### âœ… Day 1: Introduction to Airflow, DAG Basics & Scheduling

#### ðŸ“˜ What is Apache Airflow?
Apache Airflow is an open-source platform to **programmatically author, schedule, and monitor workflows**. These workflows are defined as **DAGs (Directed Acyclic Graphs)** and are useful for automating ETL, ML, and other data pipeline tasks.

---

#### ðŸ“Œ What is a DAG?
A **DAG** is a collection of tasks that run in a specific order without any cycles. Each DAG defines:
- `dag_id`: Unique identifier
- `start_date`: When the DAG starts
- `schedule_interval`: Frequency of runs (e.g., `@daily`)
- `default_args`: Default parameters for all tasks
- `tasks`: Nodes that perform specific actions (Python, Bash, etc.)

---

#### ðŸ•’ Scheduling in Airflow
- **`schedule_interval`** â€” Defines how often the DAG runs (`@daily`, `@hourly`, cron format)
- **`start_date`** â€” Sets when Airflow starts scheduling the DAG
- **`catchup=False`** â€” Prevents Airflow from running past missed schedules
- **Manual Trigger** â€” DAGs can also be run manually via the Airflow UI

---

#### ðŸ§ª Example DAG Schedule
```python
schedule_interval = "@daily"
start_date = datetime(2025, 8, 1)
